{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 42592,
     "status": "ok",
     "timestamp": 1729261369990,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "zYQgKZlprwnF",
    "outputId": "892a1564-c02a-486d-8029-d790db21e728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -U langchain langchain-openai\n",
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API: sk-p********\n",
      "LangChain API Key: lsv2********\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from supabase import create_client, Client\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY_1010\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "print(f\"OpenAI API: {openai_api_key[:4]}********\")\n",
    "print(f\"LangChain API Key: {langchain_api_key[:4]}********\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"tech4impact_1010\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "SUPABASE_URL = os.getenv(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"NEXT_PUBLIC_SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 16:17:57,272 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-96d7415e-bc7a-40e8-9426-f6af643436e6-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import getpass\n",
    "import logging \n",
    "# Set your OpenAI API key\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 16:17:57,316 - INFO - 총 403개의 문서 생성.\n"
     ]
    }
   ],
   "source": [
    "# 함수 정의: 시간 문자열을 밀리초로 변환\n",
    "def time_str_to_ms(time_str):\n",
    "    match = re.match(r'(\\d+)분 (\\d+)초', time_str)\n",
    "    if match:\n",
    "        minutes = int(match.group(1))\n",
    "        seconds = int(match.group(2))\n",
    "        return (minutes * 60 + seconds) * 1000\n",
    "    else:\n",
    "        logging.warning(f\"시간 형식이 일치하지 않습니다: {time_str}\")\n",
    "        return 0\n",
    "\n",
    "# 함수 정의: 밀리초를 시간 문자열로 변환\n",
    "def ms_to_minutes_seconds_str(milliseconds):\n",
    "    minutes = milliseconds // 60000\n",
    "    seconds = (milliseconds % 60000) // 1000\n",
    "    return f\"{minutes}분 {seconds}초\"\n",
    "\n",
    "\n",
    "with open('transcription.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "utterances = data['utterances']\n",
    "\n",
    "n = 3941239\n",
    "\n",
    "# 발화를 Document 객체로 생성\n",
    "filtered_documents = [\n",
    "    Document(\n",
    "        page_content=utterance['msg'],\n",
    "        metadata={\n",
    "            'start_at': utterance['start_at'],\n",
    "            'duration': utterance['duration'],\n",
    "            'speaker': utterance['spk'],\n",
    "            'speaker_type': utterance['spk_type']\n",
    "        }\n",
    "    )\n",
    "    for utterance in utterances if utterance['start_at'] > n\n",
    "]\n",
    "logging.info(f\"총 {len(filtered_documents)}개의 문서 생성.\")\n",
    "\n",
    "# 시작 시간 기준으로 정렬\n",
    "documents = sorted(filtered_documents, key=lambda doc: doc.metadata['start_at'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 16:17:59,519 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "full_text = ''\n",
    "for doc in documents:\n",
    "    start_time_str = ms_to_minutes_seconds_str(doc.metadata['start_at'])\n",
    "    full_text += f\"[{start_time_str}] {doc.page_content}\\n\"\n",
    "\n",
    "# 언어 모델 초기화 # mini 버전 써보기\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "# 임베딩 및 벡터스토어 초기화\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=None)\n",
    "\n",
    "# 리트리버 \n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(full_text, llm):\n",
    "    prompt = \"\"\"\n",
    "당신은 상담 세션의 스크립트에서 주요 토픽을 추출하는 AI입니다. 스크립트 내용을 기반으로 주요 토픽을 시간 순서에 따라 나열해 주세요. 각 토픽은 다음 형식으로 작성해 주세요:\n",
    "\n",
    "**토픽 1:**\n",
    "시작 시간: X분 Y초\n",
    "종료 시간: A분 B초\n",
    "내용: 주요 토픽 내용\n",
    "\n",
    "**토픽 2:**\n",
    "시작 시간: ...\n",
    "종료 시간: ...\n",
    "내용: ...\n",
    "\n",
    "스크립트:\n",
    "{text}\n",
    "\n",
    "주요 토픽 목록:\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 상담 스크립트에서 주요 토픽을 추출하는 AI입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(text=full_text)}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "\n",
    "        response = llm.invoke(messages)\n",
    "\n",
    "        topics_text = response['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # 간단한 파싱 로직\n",
    "        topics = []\n",
    "        topic_pattern = r\"토픽\\s*\\d+:\\s*시작 시간:\\s*(\\d+분 \\d+초)\\s*종료 시간:\\s*(\\d+분 \\d+초)\\s*내용:\\s*(.*)\"\n",
    "        matches = re.findall(topic_pattern, topics_text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            start_time, end_time, content = match\n",
    "            topics.append((start_time.strip(), end_time.strip(), content.strip()))\n",
    "        logging.info(f\"추출된 토픽 수: {len(topics)}\")\n",
    "        return topics\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting topics: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 16:22:16,716 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 16:22:16,720 - ERROR - Error extracting topics: 'AIMessage' object is not subscriptable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "                        model=\"gpt-4o\",\n",
    "                         temperature=0,\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "\n",
    "extract_topics(full_text,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMlVtXg7g26PnU+AN2HzD4u",
   "mount_file_id": "1gNrx_IpjBAg8HPBr_yztv8Cf-z5E8vqN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CV용 Kernel",
   "language": "python",
   "name": "cs484_hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
