{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 42592,
     "status": "ok",
     "timestamp": 1729261369990,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "zYQgKZlprwnF",
    "outputId": "892a1564-c02a-486d-8029-d790db21e728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -U langchain langchain-openai\n",
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\yooji\\anaconda3\\envs\\cs484_hw\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\yooji\\anaconda3\\envs\\cs484_hw\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/9.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.6/9.3 MB 6.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.3 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.0/9.3 MB 5.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.3.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet supabase\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2536,
     "status": "ok",
     "timestamp": 1729261372518,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "NSbgUxCIr0YY",
    "outputId": "1da637af-ac26-4255-f39b-44c71840362f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API: sk-p********\n",
      "LangChain API Key: lsv2********\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from supabase import create_client, Client\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY_1010\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "print(f\"OpenAI API: {openai_api_key[:4]}********\")\n",
    "print(f\"LangChain API Key: {langchain_api_key[:4]}********\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"tech4impact_1010\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "SUPABASE_URL = os.getenv(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"NEXT_PUBLIC_SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y83M8arbsJXm"
   },
   "source": [
    "LangSmith Setup (LLM Logging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4016,
     "status": "ok",
     "timestamp": 1729261376523,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "XKyEBNzq9F9n",
    "outputId": "9f0cc511-2df3-48cf-d0f1-eb8e2a64fccd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e10ecef1-b8ca-4f25-9e85-725a6c1b049c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-MxNp0BxHde"
   },
   "source": [
    "Starts from now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1729263038728,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "_XNUzGYHv8_t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import getpass\n",
    "import logging \n",
    "# Set your OpenAI API key\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1729261489614,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "U4H_5XiexsJR",
    "outputId": "eed29449-8112-4542-96f4-1e424ec3358b"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "TAGS = [\n",
    "    \"고지혈증 약\", \"비타민\", \"스테로이드\", \"타이레놀\",\n",
    "    \"프로바이오틱스\", \"오메가 3\", \"아르기닌\", \"유산균\",\n",
    "    \"건선\", \"고지혈증\", \"감기\", \"탈모\", \"갱년기\",\n",
    "    \"운동\", \"술자리\", \"식사 조절\", \"수면 패턴\",\n",
    "    \"간 수치\", \"가족력\", \"영양제\",\n",
    "    \"기타\"  # \"기타\" means \"Other\"\n",
    "]\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")  # Efficient embedding model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)  # Chat-based LLM for tagging\n",
    "\n",
    "MIN_LENGTH = 20\n",
    "\n",
    "# Define Semantic Merging Function Using Embeddings\n",
    "def merge_semantically(utterances, similarity_threshold=0.8):\n",
    "\n",
    "    if not utterances:\n",
    "        return []\n",
    "    \n",
    "    # Extract messages for embedding\n",
    "    messages = [utt['msg'] for utt in utterances]\n",
    "    \n",
    "    # Retrieve embeddings using LangChain\n",
    "    utterance_embeddings = embeddings.embed_documents(messages)\n",
    "    \n",
    "    merged = []\n",
    "    current = utterances[0].copy()\n",
    "    current_embedding = utterance_embeddings[0]\n",
    "    \n",
    "    for idx in range(1, len(utterances)):\n",
    "        next_utt = utterances[idx]\n",
    "        next_embedding = utterance_embeddings[idx]\n",
    "        \n",
    "        # Check if the speaker is the same\n",
    "        if current['spk'] == next_utt['spk']:\n",
    "            # Compute cosine similarity\n",
    "            similarity = cosine_similarity(\n",
    "                [current_embedding],\n",
    "                [next_embedding]\n",
    "            )[0][0]\n",
    "            \n",
    "            if similarity >= similarity_threshold:\n",
    "                # Merge utterances if similarity threshold is met\n",
    "                current['duration'] += next_utt['duration']\n",
    "                current['msg'] += \" \" + next_utt['msg']\n",
    "                # Optionally, update other metadata if needed\n",
    "            else:\n",
    "                # Add current utterance to merged list and reset\n",
    "                merged.append(current)\n",
    "                current = next_utt.copy()\n",
    "                current_embedding = next_embedding\n",
    "        else:\n",
    "            # If speakers are different, add current to merged list and reset\n",
    "            merged.append(current)\n",
    "            current = next_utt.copy()\n",
    "            current_embedding = next_embedding\n",
    "    \n",
    "    merged.append(current)\n",
    "    return merged\n",
    "def merge_short_utterances(merged_utterances, min_length=MIN_LENGTH):\n",
    "    if not merged_utterances:\n",
    "        return []\n",
    "    \n",
    "    adjusted_utterances = []\n",
    "    current = merged_utterances[0].copy()\n",
    "    \n",
    "    for next_utt in merged_utterances[1:]:\n",
    "        # Count words or characters in the next utterance\n",
    "        word_count = len(next_utt['msg'].split())\n",
    "        \n",
    "        if word_count < min_length:\n",
    "            # If the next utterance is too short, merge it with the current one\n",
    "            current['duration'] += next_utt['duration']\n",
    "            current['msg'] += \" \" + next_utt['msg']\n",
    "        else:\n",
    "            # Append the current utterance if it meets the minimum length\n",
    "            adjusted_utterances.append(current)\n",
    "            current = next_utt.copy()\n",
    "    \n",
    "    # Add the last segment\n",
    "    adjusted_utterances.append(current)\n",
    "    return adjusted_utterances\n",
    "\n",
    "\n",
    "# Define Tagging Function Using LangChain's ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def semantic_tag_utterances_batch(\n",
    "    merged_utterances: List[dict],\n",
    "    tags_list: List[str],\n",
    "    batch_size: int = 5,\n",
    "    window_size: int = 1\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Assigns semantic tags to merged utterances using LangChain's LLMChain in batches.\n",
    "    Each utterance can have multiple tags or none (defaults to '기타').\n",
    "    \n",
    "    Args:\n",
    "        merged_utterances (List[dict]): List of merged utterance dictionaries.\n",
    "        tags_list (List[str]): List of predefined tags.\n",
    "        batch_size (int, optional): Number of utterances to process in one batch. Defaults to 5.\n",
    "        window_size (int, optional): Number of previous and next utterances to include as context. Defaults to 1.\n",
    "    \n",
    "    Returns:\n",
    "        List[dict]: List of merged utterance dictionaries with assigned tags.\n",
    "    \"\"\"\n",
    "    def clean_ai_response(response: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans the AI response by removing code block markers if present.\n",
    "\n",
    "        Args:\n",
    "            response (str): The raw response from the AI.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned JSON string.\n",
    "        \"\"\"\n",
    "        # Remove triple backticks and language specifier if present\n",
    "        if response.startswith(\"```\") and \"json\" in response:\n",
    "            response = response.strip(\"```json\").strip(\"```\").strip()\n",
    "        elif response.startswith(\"```\") and \"json\" not in response:\n",
    "            response = response.strip(\"```\").strip()\n",
    "        return response\n",
    "\n",
    "    tagged_utterances = []\n",
    "    total = len(merged_utterances)\n",
    "    tags_str = \", \".join(tags_list[:-1]) + \", 기타\"\n",
    "        \n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)  # Adjust model and parameters as needed\n",
    "    prompt_template = \"\"\"\n",
    "    You are an AI assistant that assigns relevant tags to conversation fragments based on the provided list of tags.\n",
    "    Each fragment can have multiple tags or none. If no tags apply, assign '기타'.\n",
    "    Return the results as a JSON array where each element contains the 'fragment_id' and a list of assigned 'tags'.\n",
    "\n",
    "    Example Response:\n",
    "    [\n",
    "        {{\"fragment_id\": 0, \"tags\": [\"비타민\", \"운동\"]}},\n",
    "        {{\"fragment_id\": 1, \"tags\": [\"기타\"]}}\n",
    "    ]\n",
    "\n",
    "    Tags List:\n",
    "    {tags_str}\n",
    "\n",
    "    Conversation Context:\n",
    "    {chat_history}\n",
    "\n",
    "    Fragments to Tag:\n",
    "    {fragments}\n",
    "\n",
    "    Provide the assigned tags in JSON format as shown in the example.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a PromptTemplate\n",
    "    template = PromptTemplate(\n",
    "        input_variables=[\"tags_str\", \"chat_history\", \"fragments\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the LLMChain with the prompt and memory\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=template,\n",
    "    )\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = merged_utterances[i:i+batch_size]\n",
    "        \n",
    "        # Determine the context window for the entire batch\n",
    "        context_start = max(0, i - window_size)\n",
    "        context_end = min(total, i + batch_size + window_size)\n",
    "        context_utterances = merged_utterances[context_start:context_end]\n",
    "\n",
    "        # Construct the conversation context\n",
    "        conversation_context = \"\"\n",
    "        for utt in context_utterances:\n",
    "            speaker = f\"Speaker {utt['spk']}\"\n",
    "            message = utt['msg']\n",
    "            conversation_context += f\"{speaker}: {message}\\n\"\n",
    "\n",
    "        # Prepare fragments to tag with unique fragment_ids\n",
    "        fragments_info = []\n",
    "        for idx, utt in enumerate(batch):\n",
    "            fragment_id = i + idx  # Unique identifier across the entire list\n",
    "            fragments_info.append({\n",
    "                \"fragment_id\": fragment_id,\n",
    "                \"fragment\": utt['msg']\n",
    "            })\n",
    "\n",
    "        # Convert fragments_info to a JSON string for the prompt\n",
    "        fragments_json = json.dumps([\n",
    "            {\"fragment_id\": frag['fragment_id'], \"fragment\": frag['fragment']}\n",
    "            for frag in fragments_info\n",
    "        ], ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Prepare the prompt variables\n",
    "        prompt_variables = {\n",
    "            \"tags_str\": tags_str,\n",
    "            \"chat_history\": conversation_context,\n",
    "            \"fragments\": fragments_json\n",
    "        }\n",
    "\n",
    "        # Generate the prompt using the PromptTemplate\n",
    "        prompt = template.format(**prompt_variables)\n",
    "\n",
    "        try:\n",
    "            # Run the LLMChain with the prompt\n",
    "            response = chain.run(tags_str=tags_str, chat_history=conversation_context, fragments=fragments_json)\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            cleaned_response = clean_ai_response(response)\n",
    "            tags_json = json.loads(cleaned_response)\n",
    "            \n",
    "            # Create a mapping from fragment_id to tags\n",
    "            tags_mapping = {item['fragment_id']: item.get('tags', [\"기타\"]) for item in tags_json}\n",
    "            \n",
    "            for frag in fragments_info:\n",
    "                assigned_tags = tags_mapping.get(frag['fragment_id'], [\"기타\"])\n",
    "                # Ensure tags are valid\n",
    "                valid_tags = [tag for tag in assigned_tags if tag in tags_list]\n",
    "                if not valid_tags:\n",
    "                    valid_tags = [\"기타\"]\n",
    "                # Assign tags to the corresponding utterance\n",
    "                utt = merged_utterances[frag['fragment_id']]\n",
    "                utt['tags'] = valid_tags\n",
    "                tagged_utterances.append(utt)\n",
    "        \n",
    "        except json.JSONDecodeError as parse_error:\n",
    "            logging.error(f\"Error decoding JSON response: {parse_error}\")\n",
    "            logging.error(f\"AI Response: {response}\")\n",
    "            # Assign '기타' to all fragments in the batch in case of parse error\n",
    "            for frag in fragments_info:\n",
    "                utt = merged_utterances[frag['fragment_id']]\n",
    "                utt['tags'] = [\"기타\"]\n",
    "                tagged_utterances.append(utt)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during semantic tagging: {e}\")\n",
    "            # Assign '기타' to all fragments in the batch in case of API call failure\n",
    "            for frag in fragments_info:\n",
    "                utt = merged_utterances[frag['fragment_id']]\n",
    "                utt['tags'] = [\"기타\"]\n",
    "                tagged_utterances.append(utt)\n",
    "\n",
    "    return tagged_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1729266747911,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "HtokP9uDxP-O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:22:53,094 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../transcription.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "utterances = data['utterances']\n",
    "merged_utterances = merge_semantically(utterances, similarity_threshold=0.81)\n",
    "merged_utterances_short_removed =  merge_short_utterances(merged_utterances, min_length=MIN_LENGTH)\n",
    "from datetime import datetime  \n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f\"merged_utterances_{timestamp}.json\"\n",
    "\n",
    "# Save the `tagged_utterances` to a local JSON file\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_utterances, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Step 2: Tagging Merged Utterances in Batches\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:26:27,205 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:28,849 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:30,718 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:32,881 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:35,051 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:36,710 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:38,626 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:40,764 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:43,291 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:45,165 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:48,078 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:50,132 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:52,637 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:55,132 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:26:58,007 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:00,878 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:03,534 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:06,299 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:08,902 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:11,858 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:14,096 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:16,008 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:18,085 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:20,395 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:22,257 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:24,520 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:27,390 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:29,782 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:33,846 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:38,859 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:41,140 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:44,205 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:46,136 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:48,123 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:50,092 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:52,307 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:54,097 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:56,370 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:27:58,470 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:01,174 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:02,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:04,874 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:06,862 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:08,869 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:11,221 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:13,170 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:14,849 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:16,861 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:18,863 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:21,296 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:23,330 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:25,384 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:27,530 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:29,581 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:31,639 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:33,577 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:35,625 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:38,950 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:41,669 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:43,498 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:45,710 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:48,240 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:52,846 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:55,170 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:56,855 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:28:58,455 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:29:01,603 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:29:04,155 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:29:07,639 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:29:09,492 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:29:12,271 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:29:15,634 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:29:18,296 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:29:20,340 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "tagged_utterances = semantic_tag_utterances_batch(merged_utterances, TAGS, batch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 14:24:34,108 - INFO - 총 403개의 문서 생성.\n"
     ]
    }
   ],
   "source": [
    "def ms_to_minutes_seconds_str(milliseconds):\n",
    "    seconds = milliseconds // 1000  # 총 초\n",
    "    minutes = seconds // 60  # 분 계산\n",
    "    remaining_seconds = seconds % 60  # 남은 초 계산\n",
    "    return f\"{minutes}분 {remaining_seconds}초\"  # 출력 형식\n",
    "\n",
    "# 파일 \n",
    "n = 3941239\n",
    "\n",
    "# 발화를 Document 객체로 생성\n",
    "filtered_documents = [\n",
    "    Document(\n",
    "        page_content=utterance['msg'],\n",
    "        metadata={\n",
    "            'start_at': utterance['start_at'],\n",
    "            'duration': utterance['duration'],\n",
    "            'speaker': utterance['spk'],\n",
    "            'speaker_type': utterance['spk_type']\n",
    "        }\n",
    "    )\n",
    "    for utterance in utterances if utterance['start_at'] > n\n",
    "]\n",
    "logging.info(f\"총 {len(filtered_documents)}개의 문서 생성.\")\n",
    "\n",
    "# 시작 시간 기준으로 정렬\n",
    "documents = sorted(filtered_documents, key=lambda doc: doc.metadata['start_at'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7124,
     "status": "ok",
     "timestamp": 1729261501270,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "pHkD49XIySAF",
    "outputId": "d6683768-f12c-4b9c-de33-a8d51f8a81dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yooji\\AppData\\Local\\Temp\\ipykernel_7124\\454676864.py:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
      "C:\\Users\\yooji\\AppData\\Local\\Temp\\ipykernel_7124\\454676864.py:13: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "2024-11-12 13:44:23,920 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-11-12 13:44:26,360 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "full_text = ''\n",
    "for doc in documents:\n",
    "    start_time_str = ms_to_minutes_seconds_str(doc.metadata['start_at'])\n",
    "    full_text += f\"[{start_time_str}] {doc.page_content}\\n\"\n",
    "\n",
    "\n",
    "# 언어 모델 초기화 # mini 버전 써보기\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "# 임베딩 및 벡터스토어 초기화\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=None)\n",
    "\n",
    "# 리트리버 \n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1729261501271,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "LHWEiuYDyTV5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 13:56:59,903 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 13:57:11,772 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Session Summary ===\n",
      "{약: [고지혈증 약, 비타민, 스테로이드, 타이레놀, 프로바이오틱스, 오메가 3, 아르기닌, 유산균],\n",
      "질병: [건선, 고지혈증, 감기, 탈모, 갱년기],\n",
      "생활 습관: [운동, 술자리, 식사 조절, 수면 패턴],\n",
      "그 외: [간 수치, 가족력, 영양제] \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 역할 지정해서 돌려보기\n",
    "#role description\n",
    "#확정은 결과물 가지고 펠로우님이랑 이야기\n",
    "#chunk 늘려보기..\n",
    "\n",
    "prompt = \"\"\"\n",
    "        당신은 전문적인 상담 세션을 분석하고 요약하는 AI입니다. 아래 상담 세션의 전체 스크립트에서 중요한 키워드를 7~20개 추출해 주세요. 다음 지침을 반드시 따르세요:\n",
    "        \n",
    "        1. 약 이름, 환자의 질병, 생활 습관이 중요한 키워드 후보입니다. 이에 주의를 기울이세요. \n",
    "        2. 추출한 키워드를 의약품, 질병, 생활 습관, 그 외로 분류하세요.\n",
    "\n",
    "\n",
    "        **답변 예시:**\n",
    "        {{약: [타이레놀, 비타민C],\n",
    "        질병: [기립성 저혈압],\n",
    "        생활 습관: [수면 부족],\n",
    "        그 외: []\n",
    "        }}\n",
    "        **스크립트:**\n",
    "        {text}\n",
    "        \n",
    "        **요약:**\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "human_prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template=\"요약:\"\n",
    ")\n",
    "\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",  # 또는 \"map_reduce\" 등 필요에 따라 변경 가능\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "# 전체 스크립트 요약 \n",
    "response = rag_chain({\"query\": prompt.format(text=full_text)})\n",
    "session_summary = response[\"result\"]\n",
    "source_documents = response[\"source_documents\"]\n",
    "\n",
    "print(\"=== Session Summary ===\")\n",
    "print(session_summary)\n",
    "# print(\"\\n=== Source Documents ===\")\n",
    "# for doc in source_documents:\n",
    "#     print(f\"Source: {doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17465,
     "status": "ok",
     "timestamp": 1729266175924,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "KV1HpymlylqA",
    "outputId": "cfe16ae5-7192-402c-9243-530d32f5b27d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('66분 8초', '66분 42초', 'Q. 고지혈증 약을 끊었는데, 영양제를 먹어야 할까요?\\n'), ('67분 4초', '67분 19초', '[이렇게 하세요] 간 수치가 높지 않다면, 영양제를 드셔도 괜찮습니다.\\n'), ('69분 10초', '69분 15초', '[이렇게 하세요] 피로감이 있다면, 영양제를 드시는 것이 좋습니다.\\n'), ('69분 29초', '70분 40초', 'Q. 병원에서 처방받은 약과 약국에서 구입한 약의 차이는 무엇인가요?\\n'), ('71분 5초', '71분 10초', '[바꾸세요] 감기 초기에 비타민을 섭취하여 면역력을 높이세요.\\n'), ('72분 1초', '72분 17초', '[이렇게 하세요] 감기약은 증상에 맞춰서 드세요.\\n'), ('74분 2초', '74분 22초', '[이렇게 하세요] 열이 나면 타이레놀을 드세요.\\n'), ('87분 48초', '88분 26초', '[이렇게 하세요] 고지혈증 약을 오래 복용했다면 코엔자임 큐텐을 보충하세요.\\n'), ('89분 10초', '89분 19초', '[이렇게 하세요] 저녁 식사를 간단히 하고 일찍 주무세요.\\n'), ('92분 8초', '92분 50초', '[이렇게 하세요] 저녁 식사는 탄수화물보다 단백질과 채소 위주로 드세요.\\n'), ('93분 0초', '93분 34초', '[바꾸세요] 술로 인해 잠을 자는 것은 피하세요.\\n'), ('94분 8초', '94분 49초', '[이렇게 하세요] 잠들기 전에는 소음 없이 주무세요.\\n'), ('97분 0초', '97분 11초', '[이렇게 하세요] 주말에는 낮잠을 통해 피로를 푸세요.\\n'), ('98분 21초', '98분 34초', '[이렇게 하세요] 필요에 따라 영양제를 추천받아 드세요.\\n'), ('99분 8초', '99분 17초', '[이렇게 하세요] 상담 후 맞춤형 영양제를 제공받으세요.')]\n"
     ]
    }
   ],
   "source": [
    "# 출력 파싱, 주제와 시간 정보 추출\n",
    "subtopics = re.findall(r'\\*\\*(\\d+분 \\d+초) ~ (\\d+분 \\d+초)\\*\\*\\n- (.+?)(?=\\n\\*\\*\\d+분|\\Z)', session_summary, re.DOTALL)\n",
    "\n",
    "print(subtopics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1729266336389,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "XlwiQtK1yqwv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yooji\\AppData\\Local\\Temp\\ipykernel_21116\\3804599114.py:20: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(subtopic)\n",
      "2024-11-08 15:27:57,718 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:27:57,990 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:27:58,475 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:27:58,841 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:27:59,465 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:27:59,955 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:00,507 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:01,121 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:01,430 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:01,850 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:02,374 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:02,936 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:03,427 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:03,742 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 15:28:04,040 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'topic_id': 1, 'start_time': '66분 8초', 'end_time': '66분 42초', 'content': 'Q. 고지혈증 약을 끊었는데, 영양제를 먹어야 할까요?', 'related_scripts': [{'time': '66분 8초', 'content': '예, 보내주신 거는 미리 읽어보고 아는데요, 고지혈증 약만 드시고 계신 거죠.'}]}, {'topic_id': 2, 'start_time': '67분 4초', 'end_time': '67분 19초', 'content': '[이렇게 하세요] 간 수치가 높지 않다면, 영양제를 드셔도 괜찮습니다.', 'related_scripts': []}, {'topic_id': 3, 'start_time': '69분 10초', 'end_time': '69분 15초', 'content': '[이렇게 하세요] 피로감이 있다면, 영양제를 드시는 것이 좋습니다.', 'related_scripts': []}, {'topic_id': 4, 'start_time': '69분 29초', 'end_time': '70분 40초', 'content': 'Q. 병원에서 처방받은 약과 약국에서 구입한 약의 차이는 무엇인가요?', 'related_scripts': [{'time': '69분 54초', 'content': '근데 감기로만 비교를 하면 이제 그 약국에서 쓰는 약이나 병원, 약이나 차이는 그 한 가지예요.'}, {'time': '70분 28초', 'content': '예, 비슷한데 좀 그런 게 있고, 또 하나 차이는 병원에 가면 일단 직접 소독이 되잖아요.'}, {'time': '70분 21초', 'content': '되 그렇죠. 약 성분은 약국에서 그냥 파는 약이나 나머지 감기 뭐 콧물약은 비슷해요.'}, {'time': '69분 29초', 'content': '지금도 이제 간 게 기억 있는데 그냥 약국 가서 처약만 먹고 하거든요. 선생님, 근데 이게 병원에 가서 처방받는 게 나아요.'}]}, {'topic_id': 5, 'start_time': '71분 5초', 'end_time': '71분 10초', 'content': '[바꾸세요] 감기 초기에 비타민을 섭취하여 면역력을 높이세요.', 'related_scripts': []}, {'topic_id': 6, 'start_time': '72분 1초', 'end_time': '72분 17초', 'content': '[이렇게 하세요] 감기약은 증상에 맞춰서 드세요.', 'related_scripts': []}, {'topic_id': 7, 'start_time': '74분 2초', 'end_time': '74분 22초', 'content': '[이렇게 하세요] 열이 나면 타이레놀을 드세요.', 'related_scripts': [{'time': '74분 2초', 'content': '목이 부으니까 여기에 이제 임파선 같은 게 많이 있는데, 그런 림프 쪽으로 그런 그 백혈구나 림프 숫자가 급격히 늘어나면 열이 나고 에.'}, {'time': '74분 13초', 'content': '그게 사실 방어막을 부축하는 과정에서 열이 생기는 건데, 열이 나면 우리는 그 체온이 올라가면 사람이 되게 약해져요.'}]}, {'topic_id': 8, 'start_time': '87분 48초', 'end_time': '88분 26초', 'content': '[이렇게 하세요] 고지혈증 약을 오래 복용했다면 코엔자임 큐텐을 보충하세요.', 'related_scripts': [{'time': '87분 57초', 'content': '왜냐하면 또 고지혈증 약 오래 드셨으면 고 조혈증 약이 몸에서 대사될 때 이 코인자임 큐텐을 많이 소비해요.'}]}, {'topic_id': 9, 'start_time': '89분 10초', 'end_time': '89분 19초', 'content': '[이렇게 하세요] 저녁 식사를 간단히 하고 일찍 주무세요.', 'related_scripts': []}, {'topic_id': 10, 'start_time': '92분 8초', 'end_time': '92분 50초', 'content': '[이렇게 하세요] 저녁 식사는 탄수화물보다 단백질과 채소 위주로 드세요.', 'related_scripts': [{'time': '92분 8초', 'content': '예를 들어 이제 저녁에 약속이 있거나 해서 조금 더 탄수화물 위주의 식사를 한다.'}, {'time': '92분 37초', 'content': '그래서 저녁은 조금 더 고기나 채식 고기 채식으로 좀 더 가시면요.'}, {'time': '92분 28초', 'content': '그래서 전 바로 전 식사가 조금 곧 탄수화물일 때 조금 더 수면의 질이 낮아지기도 하거든요.'}]}, {'topic_id': 11, 'start_time': '93분 0초', 'end_time': '93분 34초', 'content': '[바꾸세요] 술로 인해 잠을 자는 것은 피하세요.', 'related_scripts': [{'time': '93분 11초', 'content': '그게 이제 깊은 수면으로는 못 들어가는데 잠이 들게 하는 수면 맞취 효과는 술이 확실히 있어요.'}, {'time': '93분 6초', 'content': '그랬는데 피곤한 거에다 술까지 들어가서 그냥 푹 술어도 자서 그러 건가.'}]}, {'topic_id': 12, 'start_time': '94분 8초', 'end_time': '94분 49초', 'content': '[이렇게 하세요] 잠들기 전에는 소음 없이 주무세요.', 'related_scripts': []}, {'topic_id': 13, 'start_time': '97분 0초', 'end_time': '97분 11초', 'content': '[이렇게 하세요] 주말에는 낮잠을 통해 피로를 푸세요.', 'related_scripts': [{'time': '97분 0초', 'content': '낮에 잘 수 있는 연건은 안 되니까. 약간은 좀 피곤한 상태가 조금 있는 게 아닌가 그런 거죠. 나구로 버티는 거죠.'}]}, {'topic_id': 14, 'start_time': '98분 21초', 'end_time': '98분 34초', 'content': '[이렇게 하세요] 필요에 따라 영양제를 추천받아 드세요.', 'related_scripts': [{'time': '98분 21초', 'content': '예, 저의 철학으로는 사실 약으로만 자꾸 건강관리하는 게 별로 안 좋다 생각이 들어서 저는 이렇게 상담 중에 영양제 필요하신 분 추천드리고.'}]}, {'topic_id': 15, 'start_time': '99분 8초', 'end_time': '99분 17초', 'content': '[이렇게 하세요] 상담 후 맞춤형 영양제를 제공받으세요.', 'related_scripts': []}]\n"
     ]
    }
   ],
   "source": [
    "def time_str_to_ms(time_str):\n",
    "    match = re.match(r'(\\d+)분 (\\d+)초', time_str)\n",
    "    if match:\n",
    "        minutes = int(match.group(1))\n",
    "        seconds = int(match.group(2))\n",
    "        return (minutes * 60 + seconds) * 1000\n",
    "    else:\n",
    "        logging.warning(f\"시간 형식이 일치하지 않습니다: {time_str}\")\n",
    "        return 0\n",
    "    \n",
    "json_results = []\n",
    "        \n",
    "        # 주제별 루프 처리\n",
    "for idx, (start_time_str, end_time_str, content) in enumerate(subtopics, 1):\n",
    "    subtopic = content.strip()\n",
    "    start_time_ms = time_str_to_ms(start_time_str)\n",
    "    end_time_ms = time_str_to_ms(end_time_str)\n",
    "    \n",
    "    # 해당 시간 범위 내에서 retrieve\n",
    "    retrieved_docs = retriever.get_relevant_documents(subtopic)\n",
    "    \n",
    "    relevant_docs = [\n",
    "        doc for doc in retrieved_docs\n",
    "        if start_time_ms <= doc.metadata['start_at'] <= end_time_ms\n",
    "    ]\n",
    "    \n",
    "    # 관련 스크립트 생성\n",
    "    related_scripts = [\n",
    "        {\n",
    "            \"time\": ms_to_minutes_seconds_str(doc.metadata['start_at']),\n",
    "            \"content\": doc.page_content\n",
    "        } for doc in relevant_docs\n",
    "    ]\n",
    "    \n",
    "    # 주제별 데이터를 딕셔너리로 저장\n",
    "    json_result = {\n",
    "        \"topic_id\": idx,\n",
    "        \"start_time\": start_time_str,\n",
    "        \"end_time\": end_time_str,\n",
    "        \"content\": subtopic,\n",
    "        \"related_scripts\": related_scripts\n",
    "    }\n",
    "    json_results.append(json_result)\n",
    "print(json_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1729263116942,
     "user": {
      "displayName": "유지현",
      "userId": "01868051154728111847"
     },
     "user_tz": -540
    },
    "id": "oYmGvikNBGI1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_id': 1, 'start_time': '66분 8초', 'end_time': '66분 42초', 'content': 'Q. 고지혈증 약을 끊었는데, 영양제를 먹어야 할까요?', 'related_scripts': [{'time': '66분 8초', 'content': '예, 보내주신 거는 미리 읽어보고 아는데요, 고지혈증 약만 드시고 계신 거죠.'}]}\n"
     ]
    }
   ],
   "source": [
    "print(json_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON results saved to json_results.json\n"
     ]
    }
   ],
   "source": [
    "output_filename = 'json_results.json'\n",
    "\n",
    "# Write json_results to a JSON file\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON results saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "to create docker and upload to lambda,\n",
    " docker buildx use mybuilder2\n",
    "docker buildx build --platform linux/amd64 --load -t tenten_ai_topic .\n",
    "docker tag tenten_ai_topic:latest 820604767531.dkr.ecr.ap-northeast-2.amazonaws.com/tenten_ai_topic:latest\n",
    "docker push 820604767531.dkr.ecr.ap-northeast-2.amazonaws.com/tenten_ai_topic:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If you use docker buildx, make sure to reuse the builder instance rather than creating a new one every time:\n",
    "bash\n",
    "Copy code\n",
    "# Create the builder only once\n",
    "docker buildx create --use --name mybuilder\n",
    "Then, you can use the existing builder for subsequent builds:\n",
    "bash\n",
    "Copy code\n",
    "docker buildx use mybuilder\n",
    "docker buildx build --platform linux/amd64 --load -t tenten_ai_topic .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMlVtXg7g26PnU+AN2HzD4u",
   "mount_file_id": "1gNrx_IpjBAg8HPBr_yztv8Cf-z5E8vqN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CV용 Kernel",
   "language": "python",
   "name": "cs484_hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
